model:
  hidden_size: 256
  num_hidden_layers: 4
  num_attention_heads: 4
  intermediate_size: 1024
  max_position_embeddings: 200
  dropout_rate: 0.1
  attention_dropout_rate: 0.1

training:
  batch_size: 256
  learning_rate: 1e-3
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_epochs: 100
  gradient_clip: 1.0
  use_amp: true
