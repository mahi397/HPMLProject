model:
  hidden_size: 384
  num_hidden_layers: 6
  num_attention_heads: 6
  intermediate_size: 1536
  max_position_embeddings: 200
  dropout_rate: 0.1
  attention_dropout_rate: 0.1

training:
  batch_size: 256
  learning_rate: 1e-3
  weight_decay: 0.1
  warmup_ratio: 0.1
  max_epochs: 100
  gradient_clip: 1.0
  use_amp: true
