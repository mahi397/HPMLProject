model:
  hidden_size: 768
  num_hidden_layers: 6
  num_attention_heads: 12
  intermediate_size: 3072
  max_position_embeddings: 200
  dropout_rate: 0.1
  attention_dropout_rate: 0.1

training:
  batch_size: 128  # Lower due to memory limits
  learning_rate: 1e-3
  weight_decay: 0.1
  warmup_ratio: 0.1
  max_epochs: 100
  gradient_clip: 1.0
  use_amp: true
